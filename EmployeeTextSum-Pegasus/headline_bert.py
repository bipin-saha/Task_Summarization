# -*- coding: utf-8 -*-
"""Headline_Bert.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1D-Fr2PZa2Dz9mpRjAYtzvDmTDLa21T0J
"""

! pip install transformers
#importing the libraries
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import torch
import transformers
from transformers import BertTokenizer,BertModel,AdamW,get_linear_schedule_with_warmup
import tqdm.auto as tqdm
import torch.nn.functional as F
from pylab import rcParams
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, classification_report,recall_score,precision_score,f1_score,roc_curve
from torch import nn,optim
from textwrap import wrap
from torch.utils.data import Dataset, DataLoader
from wordcloud import WordCloud
from textwrap import wrap

import matplotlib.pyplot as plt
from matplotlib import font_manager

sns.set(style='whitegrid', palette='muted', font_scale=1.2)
HAPPY_COLORS_PALETTE = ["#01BEFE", "#FFDD00", "#FF7D00", "#FF006D", "#ADFF02", "#8F00FF"]
sns.set_palette(sns.color_palette(HAPPY_COLORS_PALETTE))
rcParams['figure.figsize'] = 8, 4
RANDOM_SEED = 42
np.random.seed(RANDOM_SEED)
torch.manual_seed(RANDOM_SEED)
import warnings
warnings.filterwarnings('ignore')
!wget https://www.easynepalityping.com/resource/font/bangla/06-nikosh-bangla-font.zip
!unzip 06-nikosh-bangla-font.zip

from google.colab import drive
drive.mount('/content/drive')

df=pd.read_excel("/content/drive/MyDrive/Colab Notebooks/BAT sentiment analysis/predicted_unsupervised_sentiment.xlsx")
df.sample(10)

#check the rows and columns number.
print(f"number of rows:--> {df.shape[0]} and number of columns:--> {df.shape[1]}")

#check for missing values
df.info()

#check for missing values
def checking_m(df):
    null_v = df.isnull().sum().sort_values(ascending=False)
    null_percent = (df.isnull().sum()/df.isnull().count()).sort_values(ascending=False)
    null_v = pd.concat([null_v, null_percent], axis=1, keys=['Missing_Number', 'Missing_Percent'])
    return null_v

checking_m(df)

# Drop the first three columns
df = df[["sentence","sentiment"]]

#check for missing values
def checking_m(df):
    null_v = df.isnull().sum().sort_values(ascending=False)
    null_percent = (df.isnull().sum()/df.isnull().count()).sort_values(ascending=False)
    null_v = pd.concat([null_v, null_percent], axis=1, keys=['Missing_Number', 'Missing_Percent'])
    return null_v

checking_m(df)

#check for dublicated rows
print("Numbers of duplicated rows :",df.duplicated().sum())

#dropping the duplicated rows
df=df.drop_duplicates(keep="first")
print("After removing,now number of duplicated rows are:",df.duplicated().sum())

"""# EDA and Preprocessing"""

df.sentiment.value_counts()

#check for imblanced classes
sns.countplot(data=df,x="sentiment")
plt.xlabel('category')

import re
def replace_strings(text):
    emoji_pattern = re.compile("["
                           u"\U0001F600-\U0001F64F"  # emoticons
                           u"\U0001F300-\U0001F5FF"  # symbols & pictographs
                           u"\U0001F680-\U0001F6FF"  # transport & map symbols
                           u"\U0001F1E0-\U0001F1FF"  # flags (iOS)
                           u"\U00002702-\U000027B0"
                           u"\U000024C2-\U0001F251"
                           u"\u00C0-\u017F"          #latin
                           u"\u2000-\u206F"          #generalPunctuations

                           "]+", flags=re.UNICODE)
    english_pattern=re.compile('[a-zA-Z0-9]+', flags=re.I)
    #latin_pattern=re.compile('[A-Za-z\u00C0-\u00D6\u00D8-\u00f6\u00f8-\u00ff\s]*',)

    text=emoji_pattern.sub(r'', text)
    text=english_pattern.sub(r'', text)

    return text

def remove_punctuations(my_str):
    # define punctuation
    punctuations = '''````Â£|Â¢|Ã‘+-*/=EROeroà§³à§¦à§§à§¨à§©à§ªà§«à§¬à§­à§®à§¯012â€“34567â€¢89à¥¤!()-[]{};:'"â€œ\â€™,<>./?@#$%^&*_~â€˜â€”à¥¥â€â€°ðŸ¤£âš½ï¸âœŒï¿½ï¿°à§·ï¿°'''

    no_punct = ""
    for char in my_str:
        if char not in punctuations:
            no_punct = no_punct + char

    # display the unpunctuated string
    return no_punct

def preprocessing(text):
    out=remove_punctuations(replace_strings(text))
    return out

df['cleanText'] = df.sentence.apply(lambda x: preprocessing(str(x)))

df.sample(10)

data1 =pd.read_excel('/content/drive/MyDrive/Colab Notebooks/my_thesis/stopwords_bangla.xlsx')
stop = data1['words'].tolist()

display(data1)

def stopwordRemoval(text):
    x=str(text)
    l=x.split()

    stm=[elem for elem in l if elem not in stop]

    out=' '.join(stm)

    return str(out)

df['cleanText'] = df.cleanText.apply(lambda x: stopwordRemoval(str(x)))

#counting text length
df['counts'] = df['cleanText'].str.split().str.len()

# Remove the text with words less than 5
df= df.loc[df['counts']>1]

df.head()

df.sentiment.value_counts()

#make sure to turn on internet on your kernel
#importing stemmer
!pip install bangla-stemmer
from bangla_stemmer.stemmer import stemmer
## stemmer function
def stem_text (x):
    stmr = stemmer.BanglaStemmer()
    words=x.split(' ')
    stm = stmr.stem(words)
    words=(' ').join(stm)
    return words

df['cleanText']=df['cleanText'].apply(stem_text)

df=df[["cleanText","sentiment"]]
print(df.shape)

df.head()

#Initialize the model

PRE_TRAINED_MODEL_NAME = "sagorsarker/bangla-bert-base" #Bangla bert
tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)

#Choosing Sequence Length
#BERT works with fixed-length sequences. We'll use a simple strategy to choose the max length. Let's store the token length of each review.
token_lens = []

for txt in df.cleanText:
    tokens = tokenizer.encode(txt,max_length=512) #each row contain fixed length of 512
    token_lens.append(len(tokens))

sns.distplot(token_lens)
plt.xlim([0, 256]);
plt.xlabel('Token count')

"""# BERT MODEL"""

#split the data
df_train, df_test = train_test_split(df,test_size=0.2, random_state=RANDOM_SEED)
df_val, df_test = train_test_split(df_test, test_size=0.5, random_state=RANDOM_SEED)
print("train shape:",df_train.shape)
print("test shape:",df_test.shape)
print("val shape:",df_val.shape)

class_names = ['negative','positive']

#---------------------------Create dataset------------------------------------

MAX_LEN = 70

class SentimentClassifierDataset(Dataset):
    def __init__(self, text, targets2, tokenizer, max_len):
        self.text = text
        self.targets2 = targets2
        self.tokenizer = tokenizer
        self.max_len = max_len

    def __len__(self):
        return len(self.text)

    def __getitem__(self, index):
        text = str(self.text[index])
        target2 = self.targets2[index]

        encoding = self.tokenizer.encode_plus(
          text,
          add_special_tokens=True,
          max_length=self.max_len,
          return_token_type_ids=False,
          pad_to_max_length=True,
          return_attention_mask=True,
          return_tensors='pt',
        )

        return {
          'Headline_text':text,
          'input_ids': encoding['input_ids'].flatten(),
          'attention_mask': encoding['attention_mask'].flatten(),
          'targets': torch.tensor(target2, dtype=torch.long)
                }

#--------------------------Create Data Loader-------------------------------

def create_data_loader2(df, tokenizer, max_len, batch_size):
    #create a instance of the SentimentClassifierDataset
    ds = SentimentClassifierDataset(
        text=df.cleanText.to_numpy(),
        targets2=df.sentiment.to_numpy(),
        tokenizer=tokenizer,
        max_len=max_len)

    return DataLoader(
        ds,
        batch_size=batch_size)

BATCH_SIZE = 64

train_data_loader2 = create_data_loader2(df_train, tokenizer, MAX_LEN, BATCH_SIZE)
val_data_loader2 = create_data_loader2(df_val, tokenizer, MAX_LEN, BATCH_SIZE)
test_data_loader2 = create_data_loader2(df_test, tokenizer, MAX_LEN, BATCH_SIZE)

#--------------------------------Building the model--------------------------------

#initialize the model
bert_model = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME,return_dict=False)

class SentimentClassifier(nn.Module):

    def __init__(self, n_classes):
        super(SentimentClassifier, self).__init__()
        self.bert = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME,return_dict=False)
        self.drop = nn.Dropout(0.5)
        self.out = nn.Linear(self.bert.config.hidden_size, n_classes)

    def forward(self, input_ids, attention_mask):

        _, pooled_output = self.bert(
        input_ids=input_ids,
        attention_mask=attention_mask,return_dict=False)
        output = self.drop(pooled_output)
        return self.out(output)

model2 = SentimentClassifier(len(class_names))
#model = model.to(device)

#-----------------------------------Training----------------------------------

EPOCHS = 5

optimizer = AdamW(model2.parameters(), lr=2e-5, correct_bias=False)
total_steps = len(train_data_loader2) * EPOCHS

scheduler = get_linear_schedule_with_warmup(
  optimizer,
  num_warmup_steps=0,
  num_training_steps=total_steps
)

loss_fn = nn.CrossEntropyLoss()

# ----------------------------------Traning function------------------------------------
def train_epoch2(model2,
                data_loader,
                loss_fn,
                optimizer,
                scheduler,
                n_examples):

    model2 = model2.train() # model becoming in the training mode.

    losses = []
    correct_predictions = 0

    for batch in data_loader:
        input_ids = batch["input_ids"]
        attention_mask = batch["attention_mask"]
        targets = batch["targets"]

        outputs = model2(
          input_ids=input_ids,
          attention_mask=attention_mask)

        _, preds = torch.max(outputs, dim=1)
        loss = loss_fn(outputs, targets)

        correct_predictions += torch.sum(preds == targets)
        losses.append(loss.item())

        loss.backward()
        #avoiding the exploiding gradiant
        nn.utils.clip_grad_norm_(model2.parameters(), max_norm=1.0)
        optimizer.step()
        scheduler.step()
        optimizer.zero_grad()

    return correct_predictions.double() / n_examples, np.mean(losses)

# ------------------------------Evaluation function-----------------------------------
def eval_model2(model2, data_loader, loss_fn, n_examples):

    model2 = model2.eval() # model becoming in the evaluation mode.(no dropout,batch norm apply)

    losses = []
    correct_predictions = 0

    with torch.no_grad():
        for batch in data_loader:
            input_ids = batch["input_ids"]
            attention_mask = batch["attention_mask"]
            targets = batch["targets"]

            outputs = model2(
                input_ids=input_ids,
                attention_mask=attention_mask)
            _, preds = torch.max(outputs, dim=1)

            loss = loss_fn(outputs, targets)

            correct_predictions += torch.sum(preds == targets)
            losses.append(loss.item())

        return correct_predictions.double() / n_examples, np.mean(losses)

# starting the training process.
from collections import defaultdict
history = defaultdict(list)
best_accuracy = 0

for epoch in range(EPOCHS):

    print(f'Epoch {epoch + 1}/{EPOCHS}')
    print('-' * 30)

    train_acc, train_loss = train_epoch2(
        model2,
        train_data_loader2,
        loss_fn,
        optimizer,
        scheduler,
        len(df_train))

    print(f'Train loss {train_loss} accuracy {train_acc}')

    val_acc, val_loss = eval_model2(
        model2,
        val_data_loader2,
        loss_fn,
        len(df_val))

    print(f'Val  loss {val_loss} accuracy {val_acc}')
    print()

    history['train_acc'].append(train_acc)
    history['train_loss'].append(train_loss)
    history['val_acc'].append(val_acc)
    history['val_loss'].append(val_loss)

    if val_acc > best_accuracy:
        torch.save(model2.state_dict(), '/content/drive/MyDrive/Colab Notebooks/my_thesis/best_model_state_BAT.bin')
        best_accuracy = val_acc





